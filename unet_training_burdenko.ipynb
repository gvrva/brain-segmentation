{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "glmeEIAUNufG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetBrats(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms):\n",
    "        self.root = 'burdenko_numpy/'\n",
    "        unnecessary_files = {'.DS_Store', '.ipynb_checkpoints'}\n",
    "        self.df = pd.read_csv(f'burdenko_numpy/burdenko_slices.csv')\n",
    "        self.group1 = {'1019_18','1034_18_4','1036_18','1043_18_4','1056_18_4','1072_19','1112_19_4','1159_18_4',\n",
    "                  '1164_18','1170_18_4','1184_18','1185_18_4','1214_18','1257_18','1267_18_4','1275_19_4', '1333_18',\n",
    "                  '1357_19_4','1484_18_4','1541_18_4','1546_18','1733_18','1734_18','1781_18','1795_18_', '255_18',\n",
    "                  '608_18_4','664_18_4','672_18_4','705_18_4','746_19_4','788_18','826_18_4','856_19_4', '923_18',\n",
    "                  '925_18_4','946_18','979_18_4'}\n",
    "\n",
    "        self.group2 = {'1028_18_4','1096_18','1216_18','1254_18','1255_18','1258_18','1302_18_4','1326_18','1354_18_4',\n",
    "                  '1360_18','1362_18_4','1421_18','1463_18_4','1470_18_4','1501_18_4','1515_18_4','1539_18',\n",
    "                  '1566_18','1573_18_4','1635_18','1646_18','1685_18_4','1702_18','1743_18_4','1746_18_4',\n",
    "                  '1764_18_4','1769_18','1770_18_4','322_18_4','349_18_4','351_18','541_18','558_18_4','573_18_4',\n",
    "                  '575_18_4','593_18','644_19_4','660_18_4','745_18_4','770_18','875_18_4','971_18_4','990_18_4',\n",
    "                  'Patient_1000314','Patient_1000815','Patient_1001316','Patient_102117','Patient_103717',\n",
    "                  'Patient_104514','Patient_105215','Patient_107017','Patient_109017','Patient_109414',\n",
    "                  'Patient_110014','Patient_110816','Patient_111016','Patient_120115','Patient_12115',\n",
    "                  'Patient_12214','Patient_122315','Patient_123816','Patient_12417','Patient_127916',\n",
    "                  'Patient_129316','Patient_129415','Patient_129816','Patient_130514','Patient_131416',\n",
    "                  'Patient_132216','Patient_133916','Patient_135915','Patient_136415','Patient_136715',\n",
    "                  'Patient_136915','Patient_137315','Patient_138316','Patient_138516','Patient_140316',\n",
    "                  'Patient_146716','Patient_15215','Patient_15817','Patient_158716','Patient_161316',\n",
    "                  'Patient_1815','Patient_20717','Patient_22117','Patient_24117','Patient_24717', 'Patient_24815',\n",
    "                  'Patient_28514','Patient_2914','Patient_33217','Patient_43316', 'Patient_43515','Patient_45217',\n",
    "                  'Patient_48417','Patient_48517','Patient_49617', 'Patient_5117','Patient_51815','Patient_52315',\n",
    "                  'Patient_54317','Patient_56717', 'Patient_59315','Patient_59817','Patient_61715','Patient_61916',\n",
    "                  'Patient_62315', 'Patient_62817','Patient_65516','Patient_66615','Patient_69515','Patient_70614',\n",
    "                  'Patient_716','Patient_72715','Patient_74417','Patient_75116','Patient_76516', 'Patient_8017',\n",
    "                  'Patient_83217','Patient_83714','Patient_84116','Patient_87114', 'Patient_88817','Patient_88917',\n",
    "                  'Patient_89117','Patient_90517','Patient_90616', 'Patient_92114','Patient_9315','Patient_95717',\n",
    "                  'Patient_98814','Patient_98817','Patient_99715'}\n",
    "\n",
    "        self.group3 = {'1029_18_4','1744_18','1765_18_4','1788_18_4','423_18','607_18','668_18_4','688_18'}\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        folder, slice_ = self.df.iloc[idx][['patient', 'slice']]\n",
    "        if (folder in self.group1) or (folder in self.group3):\n",
    "            image = np.load(f'{self.root}{folder}/flair.npz')['arr_0'][:, slice_, :]\n",
    "        if folder in self.group2:\n",
    "            image = np.load(f'{self.root}{folder}/flair.npz')['arr_0'][:, :, slice_]\n",
    "        image = image / np.max(image) * 255\n",
    "        \n",
    "        if (folder in self.group1) or (folder in self.group3):\n",
    "            mask = np.load(f'{self.root}{folder}/mask.npz')['arr_0'][:, slice_, :]\n",
    "        if folder in self.group2:\n",
    "            mask = np.load(f'{self.root}{folder}/mask.npz')['arr_0'][:, :, slice_]\n",
    "        mask[mask > 1] = 1\n",
    "        transformed = self.transforms(image=np.array(image, dtype = np.uint8),\n",
    "                                      mask=np.array(mask, dtype = np.uint8))\n",
    "        image = transformed[\"image\"].float()\n",
    "        mask = transformed[\"mask\"].float().unsqueeze(0)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "brE9OO5WN-x2"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': A.Compose(\n",
    "        [\n",
    "        A.RandomResizedCrop(256, 256, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=0.3),\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Normalize(mean=0, std=1),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    ),\n",
    "    'val': A.Compose(\n",
    "        [\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(mean=0, std=1),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "41PKL4rKN-0V"
   },
   "outputs": [],
   "source": [
    "dataset_train = ImageDatasetBrats(data_transforms['train'])\n",
    "dataset_test = ImageDatasetBrats(data_transforms['val'])\n",
    "\n",
    "torch.manual_seed(123) #для воспроизводимости\n",
    "indices = torch.randperm(180).tolist()\n",
    "t = int(180*0.7)\n",
    "\n",
    "df_for_training = pd.read_csv(\"burdenko_numpy/burdenko_slices.csv\")\n",
    "\n",
    "train_indices = df_for_training[df_for_training.patient_index.isin(indices[:t])]['index'].tolist()\n",
    "test_indices = df_for_training[df_for_training.patient_index.isin(indices[t:])]['index'].tolist()\n",
    "\n",
    "dataset_train = torch.utils.data.Subset(dataset_train, train_indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=4),\n",
    "               'test': torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=4)}\n",
    "\n",
    "dataset_sizes = {'train': len(dataset_train), 'val': len(dataset_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 21 23:05:30 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n",
      "| 32%   65C    P2    90W / 250W |   2213MiB /  6144MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1663      G   /usr/lib/xorg/Xorg                 48MiB |\r\n",
      "|    0   N/A  N/A     28859      C   ...rova/anaconda3/bin/python     1068MiB |\r\n",
      "|    0   N/A  N/A     30315      C   ...rova/anaconda3/bin/python     1090MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X7UTkbDUN-6E"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6VVr9FWfN30v"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=1, features=32, dropout=False, pooling_size=2):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        if dropout:\n",
    "            dropout_layer = nn.Dropout(0.1)\n",
    "        else:\n",
    "            dropout_layer = nn.Identity()\n",
    "\n",
    "        self.init_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(features, features, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features, features, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features, features, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.shortcut0 = nn.Conv2d(features, features, kernel_size=1)\n",
    "\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.Conv2d(features, features * 2, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer,\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.shortcut1 = nn.Conv2d(features * 2, features * 2, 1)\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.Conv2d(features * 2, features * 4, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer,\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.shortcut2 = nn.Conv2d(features * 4, features * 4, 1)\n",
    "\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.Conv2d(features * 4, features * 8, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer,\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            dropout_layer\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.Sequential(\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 8, features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(features * 8),\n",
    "            nn.ConvTranspose2d(features * 8, features * 4, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 4, features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.ConvTranspose2d(features * 4, features * 2, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1),\n",
    "            ResidualBlock(features * 2, features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.ConvTranspose2d(features * 2, features, kernel_size=pooling_size, stride=pooling_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            dropout_layer\n",
    "        )\n",
    "\n",
    "        self.out_path = nn.Sequential(\n",
    "            ResidualBlock(features, features, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(features, out_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.init_path(x)\n",
    "        x1 = self.down1(x0)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "\n",
    "        x2_up = self.up3(x3)\n",
    "        x1_up = self.up2(x2_up + self.shortcut2(x2))\n",
    "        x0_up = self.up1(x1_up + self.shortcut1(x1))\n",
    "        x_out = self.out_path(x0_up + self.shortcut0(x0))\n",
    "        return x_out\n",
    "#         return torch.sigmoid(x_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C7E4Z1-PuZwe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/i_govorova/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "# model = ResUNet(in_channels=1, out_channels=4)\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=1, out_channels=1, init_features=32, pretrained=False)\n",
    "model.load_state_dict(torch.load(\"model_weights/unet_burd_orig.pth\", map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "model_name = \"model_weights/unet_burd_orig_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_metric(pred, label):\n",
    "    intersection = 2.0 * (pred * label).sum()\n",
    "    union = pred.sum() + label.sum()\n",
    "    if pred.sum() == 0 and label.sum() == 0:\n",
    "        return 1.\n",
    "    return intersection / union\n",
    "\n",
    "def dice_coef_loss(pred, label):\n",
    "    smooth = 1.0\n",
    "    intersection = 2.0 * (pred * label).sum() + smooth\n",
    "    union = pred.sum() + label.sum() + smooth\n",
    "    return 1 - (intersection / union)\n",
    "\n",
    "def train_loop(model, loader, loss_func):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_dices = []\n",
    "    \n",
    "    for image, mask in tqdm(loader):\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        outputs = model(image)\n",
    "        out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0            \n",
    "\n",
    "        dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n",
    "        loss = loss_func(outputs, mask)\n",
    "        train_losses.append(loss.item())\n",
    "        train_dices.append(dice)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return train_dices, train_losses\n",
    "\n",
    "def eval_loop(model, loader, loss_func, epoch, best_dice, training=True):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (image, mask) in tqdm(enumerate(loader)):\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "    \n",
    "            outputs = model(image)\n",
    "            loss = loss_func(outputs, mask)\n",
    "            \n",
    "            out_cut = np.copy(outputs.data.cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_dice += dice\n",
    "        \n",
    "        val_mean_dice = val_dice / step\n",
    "        val_mean_loss = val_loss / step\n",
    "        \n",
    "        if val_mean_dice > best_dice:\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, f'{model_name}.pth')      \n",
    "        \n",
    "        if training:\n",
    "            scheduler.step(val_mean_dice)\n",
    "        \n",
    "    return val_mean_dice, val_mean_loss\n",
    "\n",
    "def train_model(train_loader, val_loader, loss_func, optimizer, scheduler, num_epochs):\n",
    "    train_loss_history = []\n",
    "    train_dice_history = []\n",
    "    val_loss_history = []\n",
    "    val_dice_history = []\n",
    "    val_mean_dice = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_dices, train_losses = train_loop(model, train_loader, loss_func)\n",
    "        train_mean_dice = np.array(train_dices).mean()\n",
    "        train_mean_loss = np.array(train_losses).mean()\n",
    "        val_mean_dice, val_mean_loss = eval_loop(model, val_loader, loss_func, epoch, val_mean_dice)\n",
    "        \n",
    "        train_loss_history.append(np.array(train_losses).mean())\n",
    "        train_dice_history.append(np.array(train_dices).mean())\n",
    "        val_loss_history.append(val_mean_loss)\n",
    "        val_dice_history.append(val_mean_dice)\n",
    "        \n",
    "        print('Epoch: {}/{} |  Train Loss: {:.3f}, Val Loss: {:.3f}, Train DICE: {:.3f}, Val DICE: {:.3f}'.format(epoch+1, num_epochs,\n",
    "                                                                                                                 train_mean_loss,\n",
    "                                                                                                                 val_mean_loss,\n",
    "                                                                                                                 train_mean_dice,\n",
    "                                                                                                                 val_mean_dice))\n",
    "        \n",
    "\n",
    "    return train_loss_history, train_dice_history, val_loss_history, val_dice_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 896/896 [28:06<00:00,  1.88s/it]\n",
      "234it [05:49,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/11 |  Train Loss: 0.203, Val Loss: 0.413, Train DICE: 0.797, Val DICE: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████████████▍                                                    | 500/896 [15:16<09:30,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n",
    "train_loss_history, train_dice_history, val_loss_history, val_dice_history = train_model(dataloaders['train'],\n",
    "                                                                                         dataloaders['test'],\n",
    "                                                                                         dice_coef_loss, \n",
    "                                                                                         optimizer,\n",
    "                                                                                         scheduler, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "unet_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
